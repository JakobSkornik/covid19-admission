{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting COVID-19 ICU Admission Using Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "The original dataset is under **Attribution-NonCommercial 4.0 (International CC BY-NC 4.0)** license.\n",
    "\n",
    "Dataset is free to **share** and **adapt** under the following terms:\n",
    "\n",
    "* credit to the original article is given and any changes are indicated (nothing has been changed as of 30/11/2021)\n",
    "* material is not used for commercial purposes \n",
    "\n",
    "Credit:\n",
    "\n",
    "* original material is published on [Kaggle](https://www.kaggle.com/) and accessible [here](https://www.kaggle.com/S%C3%ADrio-Libanes/covid19)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "This repository contains source code and report for a seminar paper in the context of the course *Machine Learning* in the winter semester 2021/2021 at Faculty of Computer and Information science, University of Ljubljana.\n",
    "\n",
    "The dataset contains anonymized data from Hospital Sírio-Libanês, São Paulo and Brasilia. \n",
    "\n",
    "### Context (*copied from the above-mentioned Kaggle article*)\n",
    "COVID-19 pandemic impacted the whole world, overwhelming healthcare systems - unprepared for such intense and lengthy request for ICU beds, professionals, personal protection equipment and healthcare resources.\n",
    "Brazil recorded first COVID-19 case on February 26 and reached community transmission on March 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Predict admission to the ICU of confirmed COVID-19 cases.\n",
    "Based on the data available, is it feasible to predict which patients will need intensive care unit support?\n",
    "The aim is to provide tertiary and quarternary hospitals with the most accurate answer, so ICU resources can be arranged or patient transfer can be scheduled (*copied from Kaggle article*).\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Data has been cleaned and scaled by column according to Min Max Scaler. In total, there are 54 features (expanded when pertinent to the mean, median, max, min, diff and relative diff). \n",
    "\n",
    "### Available Data\n",
    "\n",
    "Features in the dataset can be grouped in four groups.\n",
    "\n",
    "\n",
    "\n",
    "| Group | Amount of features |\n",
    "| ----- | :------------------: |\n",
    "| Demographics | 3 |\n",
    "| Grouped diseases | 9 |\n",
    "| Blood results | 36 |\n",
    "| Vital signs | 6 |\n",
    "| **Total**| **54** |\n",
    "\n",
    "\n",
    "\n",
    "### Window Concept\n",
    "\n",
    "Data for each patient has been grouped in five windows, each containing diagnostic results from the respective time window.\n",
    "\n",
    "\n",
    "\n",
    "| Window      | Description |\n",
    "| ----------- | ----------- |\n",
    "| 0-2         | From 0 to 2 hours of the admission |\n",
    "| 2-4         | From 2 to 4 hours of the admission |\n",
    "| 4-6         | From 4 to 6 hours of the admission |\n",
    "| 6-12        | From 6 to 12 hours of the admission |\n",
    "| Above 12    | Above 12 hours from admission |\n",
    "\n",
    "\n",
    "\n",
    "Kaggle article warns not to use data from the window where the target variable is 1. This means we need to manipulate our data a little. For example let's take a look at the following time tables:\n",
    "\n",
    "\n",
    "\n",
    "| Window      | Patient admitted to ICU | Data can be used for modelling | Target variable |\n",
    "| ----------- | :-----------: | :-----------: | :-----------: |\n",
    "| 0-2         | False | True | 1 |\n",
    "| 2-4         | False | True | 1 |\n",
    "| 4-6         | False | True | 1 |\n",
    "| 6-12        | True | False |  |\n",
    "| Above 12    | True | False |  |\n",
    "\n",
    "\n",
    "\n",
    "Patient is admitted in the fourth time window (6-12 from initial non-ICU admission). This means we can use data from the first three time windows with target variable being 1 (patient being admitted to the ICU ward).\n",
    "\n",
    "\n",
    "\n",
    "| Window      | Patient admitted to ICU | Data can be used for modelling | Target variable |\n",
    "| ----------- | :-----------: | :-----------: | :-----------: |\n",
    "| 0-2         | False | True | 0 |\n",
    "| 2-4         | False | True | 0 |\n",
    "| 4-6         | False | True | 0 |\n",
    "| 6-12        | False | True | 0 |\n",
    "| Above 12    | False | True | 0 |\n",
    "\n",
    "\n",
    "\n",
    "Patient is never admitted to the ICU, we can therefore use all time windows with target variable 0.\n",
    "\n",
    "### Null Values\n",
    "\n",
    "If we take a look at the following snippet from the original Kaggle article:\n",
    "\n",
    "```\n",
    "It is reasonable to assume that a patient who does not have a measurement recorded in a time window is clinically stable, potentially presenting vital signs and blood labs similar to neighboring windows. Therefore, one may fill the missing values using the next or previous entry. Attention to multicollinearity and zero variance issues in this data when choosing your algorithm.\n",
    "```\n",
    "\n",
    "We will be filling missing values from neighbouring cells, as specified in the snippet above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Before you begin to run your code, you need to load all required modules. Simply execute the code block below. This block also enables Jupyter's auto-reloading feature, so you dont need to re-import modules whenever you change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# In order to import from the python file without hassle, we add the current\n",
    "# directory to the python path\n",
    "import sys; sys.path.append(\".\")\n",
    "\n",
    "# Auto-reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Utilities module\n",
    "import src.utilities as util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Instructions on how to set up the environment are specified in the [README](https://github.com/JakobSkornik/covid19-admission/blob/main/README.md) file.\n",
    "\n",
    "The original dataset is provided in a single *xlsx* file. Let us first import the dataset and store it in a single pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Excel into DataFrame\n",
    "dataset = util.load_xlsx(\"data/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx\")\n",
    "\n",
    "# Print first 10 elements of DataFrame\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the datatypes present in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we want to do, is to add the target variable to all the rows. If there's at least one positive value in the **ICU** column for a single patient, the target variable is 1.\n",
    "\n",
    "First we obtain the target variable for every patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df with PATIENT_ID/TARGET columns\n",
    "patient_target_df = util.get_target_variables(dataset)\n",
    "patient_target_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we append target variable to each row of the **dataset** dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = util.append_target_variable(dataset, patient_target_df)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can remove the rows, where dataset contains value 1 in the column **ICU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset.ICU != 1]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we remove metadata column containing patient ID **PATIENT_VISIT_IDENTIFIER** and column **ICU**, since every row has the same ICU value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop([\"PATIENT_VISIT_IDENTIFIER\", \"ICU\"], axis=1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have to deal with null values. As specified above, we will fill null values with neighbouring values. We can take advantage of the **pd.DataFrame.fillna** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_backward_fill = dataset.fillna(method=\"bfill\")\n",
    "dataset_forward_fill = dataset.fillna(method=\"ffill\")\n",
    "\n",
    "backward_filled = dataset_backward_fill.isna().sum().all()\n",
    "forward_filled = dataset_forward_fill.isna().sum().all()\n",
    "\n",
    "backward_filled, forward_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods successfully filled the dataset. We can select either one of those, or save them to compare results between them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_backward_fill\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still whitespace characters in column names, so we replace them with underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns = dataset.columns.str.replace(\" \", \"_\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to encode the **AGE_PERCENTIL** column. We can see that there are 10 distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.AGE_PERCENTIL.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use pandas.get_dummies method, which will map a single column with n possible values, into n different binary columns. Column representing the original value, will contain value 1.\n",
    "\n",
    "For example the following column:\n",
    "\n",
    "| Value |\n",
    "| :----: |\n",
    "| a |\n",
    "| b |\n",
    "| c |\n",
    "| a |\n",
    "| a |\n",
    "\n",
    "will get mapped into:\n",
    "\n",
    "| Value_a | Value_b | Value_c |\n",
    "| :---: | :---: | :---: |\n",
    "| 1 | 0 | 0 |\n",
    "| 0 | 1 | 0 |\n",
    "| 0 | 0 | 1 |\n",
    "| 1 | 0 | 0 |\n",
    "| 1 | 0 | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = util.get_dummies(dataset, cols=[\"AGE_PERCENTIL\"])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final non-numeric column is the **WINDOW** column. Using this column, we can create 6 different datasets. There are five windows, so we can use each distinct value as a separate dataset and an additional dataset with all time windows. I will show the example for window 2-4. Keep in mind, when we create a dataset for a window, all previous windows must be included aswell.\n",
    "\n",
    "After we extract a dataset for a desired window, **WINDOW** column can be dropped.\n",
    "\n",
    "All datasets will then be created and stored in a new object, with all required metadata using a helper method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_24_dataset = dataset[(dataset.WINDOW == \"0-2\") | (dataset.WINDOW == \"2-4\")]\n",
    "window_24_dataset = window_24_dataset.drop(\"WINDOW\", axis=1)\n",
    "window_24_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make sure that the datatypes of the curated dataset are all numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_24_dataset.dtypes.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply this data preparation again, this time for all time windows and for different fill methods separately in a script. The final dictionary is structured as follows:\n",
    "\n",
    "* **datasets**: *dict*\n",
    "  * **ffill_datasets**: *dict*\n",
    "    * **window_0_2**: *pd.Dataframe*\n",
    "    * **window_2_4**: *pd.DataFrame*\n",
    "    * **window_4_6**: *pd.DataFrame*\n",
    "    * **window_6_12**: *pd.DataFrame*\n",
    "    * **window_all**: *pd.DataFrame*\n",
    "  * **bfill_datasets**: *dict*\n",
    "    * . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = util.get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "The first question as to why one would want to design a neural network by hand, can be answered easily; to understand how such algorithms work at a deep level and because it's fun.\n",
    "\n",
    "The whole neural network is designed as a package, that we can use in this notebook. We will start with a simple stochastic gradient descent backpropagation algorithm.\n",
    "\n",
    "The first step is to desgin a simple neural network thats capable of approximating some basic boolean functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to import from the python file without hassle, we add the current\n",
    "# directory to the python path\n",
    "import sys; sys.path.append(\".\")\n",
    "\n",
    "# Auto-reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.neural_network.basic import BasicNeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss: 0.7362408378181899, accuracy: 0.75, learning_rate: 1.0\n",
      "20: loss: 0.08632305650808583, accuracy: 1.0, learning_rate: 0.9803921568627451\n",
      "40: loss: 0.02079537572234387, accuracy: 1.0, learning_rate: 0.9615384615384615\n",
      "60: loss: 0.010269519290616, accuracy: 1.0, learning_rate: 0.9433962264150942\n",
      "80: loss: 0.006620037406907475, accuracy: 1.0, learning_rate: 0.9259259259259258\n",
      "100: loss: 0.0047786532429224884, accuracy: 1.0, learning_rate: 0.9090909090909091\n",
      "120: loss: 0.003699604081862335, accuracy: 1.0, learning_rate: 0.8928571428571428\n",
      "140: loss: 0.0030146116628634546, accuracy: 1.0, learning_rate: 0.8771929824561403\n",
      "160: loss: 0.0025307760151538467, accuracy: 1.0, learning_rate: 0.8620689655172414\n",
      "180: loss: 0.002182254365437768, accuracy: 1.0, learning_rate: 0.8474576271186441\n",
      "Finished learning. Accuracy: 1.0.\n",
      "=======================\n",
      "RESULTS:\n",
      "\n",
      "    TP: 1,\n",
      "    TN: 3,\n",
      "    FP: 0,\n",
      "    FN: 0\n",
      "    accuracy: 4 / 4 = 100.0%\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Create a neural network\n",
    "basic5_AND = BasicNeuralNetwork(\n",
    "    input_size=2,\n",
    "    output_size=2,\n",
    "    iterations=200,\n",
    "    logs=True,\n",
    "    log_frequency=20,\n",
    "    alpha=1,\n",
    "    alpha_decay=0.001,\n",
    "    layer_size=5\n",
    ")\n",
    "\n",
    "# Create dataset\n",
    "X = [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "]\n",
    "\n",
    "# AND target values\n",
    "y = [0, 0, 0, 1]\n",
    "\n",
    "# Train on the dataset\n",
    "basic5_AND.learn(X, y)\n",
    "\n",
    "# Evaluate model\n",
    "basic5_AND_confusion_matrix = util.evaluate_custom(X, y, basic5_AND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network seems to work for an AND method. Let's try XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss: 0.6645764462484502, accuracy: 0.75, learning_rate: 1.0\n",
      "20: loss: 0.17532361130278246, accuracy: 1.0, learning_rate: 0.9803921568627451\n",
      "40: loss: 0.045186423592068856, accuracy: 1.0, learning_rate: 0.9615384615384615\n",
      "60: loss: 0.024914716906131618, accuracy: 1.0, learning_rate: 0.9433962264150942\n",
      "80: loss: 0.017501967144621735, accuracy: 1.0, learning_rate: 0.9259259259259258\n",
      "100: loss: 0.012952949764513205, accuracy: 1.0, learning_rate: 0.9090909090909091\n",
      "120: loss: 0.010325487554211317, accuracy: 1.0, learning_rate: 0.8928571428571428\n",
      "140: loss: 0.008632486974339897, accuracy: 1.0, learning_rate: 0.8771929824561403\n",
      "160: loss: 0.007429856751153955, accuracy: 1.0, learning_rate: 0.8620689655172414\n",
      "180: loss: 0.006409485392622069, accuracy: 1.0, learning_rate: 0.8474576271186441\n",
      "Finished learning. Accuracy: 1.0.\n",
      "=======================\n",
      "RESULTS:\n",
      "\n",
      "    TP: 2,\n",
      "    TN: 2,\n",
      "    FP: 0,\n",
      "    FN: 0\n",
      "    accuracy: 4 / 4 = 100.0%\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Create a neural network\n",
    "basic5_XOR = BasicNeuralNetwork(\n",
    "    input_size=2,\n",
    "    output_size=2,\n",
    "    iterations=200,\n",
    "    logs=True,\n",
    "    log_frequency=20,\n",
    "    alpha=1,\n",
    "    alpha_decay=0.001,\n",
    "    layer_size=5\n",
    ")\n",
    "\n",
    "# Create dataset\n",
    "X = [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "]\n",
    "\n",
    "# XOR target values\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "# Train on the dataset\n",
    "basic5_XOR.learn(X, y)\n",
    "\n",
    "# Evaluate model\n",
    "basic5_XOR_confusion_matrix = util.evaluate_custom(X, y, basic5_XOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our neural network on the actual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[\"window_all\"].copy()\n",
    "\n",
    "X = dataset.drop(\"TARGET\", axis=1).to_numpy()\n",
    "\n",
    "y = dataset[[\"TARGET\"]].to_numpy()\n",
    "y = y.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss: 7.745457238201585, accuracy: 0.3262411347517731, learning_rate: 1.0\n",
      "200: loss: 0.6314844496650722, accuracy: 0.6737588652482269, learning_rate: 0.8333333333333334\n",
      "400: loss: 0.6314844496650722, accuracy: 0.6737588652482269, learning_rate: 0.7142857142857143\n",
      "600: loss: 0.6314844496650722, accuracy: 0.6737588652482269, learning_rate: 0.625\n",
      "800: loss: 0.6314844496650722, accuracy: 0.6737588652482269, learning_rate: 0.5555555555555556\n",
      "1000: loss: 0.6314844496650722, accuracy: 0.6737588652482269, learning_rate: 0.5\n",
      "1200: loss: 0.6314844496650722, accuracy: 0.6737588652482269, learning_rate: 0.45454545454545453\n",
      "1400: loss: 0.6314844496650722, accuracy: 0.6737588652482269, learning_rate: 0.41666666666666663\n",
      "1600: loss: 0.6314844496650722, accuracy: 0.6737588652482269, learning_rate: 0.3846153846153846\n",
      "1800: loss: 0.6314844496650723, accuracy: 0.6737588652482269, learning_rate: 0.35714285714285715\n",
      "Finished learning. Accuracy: 0.6737588652482269.\n",
      "=======================\n",
      "RESULTS:\n",
      "\n",
      "    TP: 0,\n",
      "    TN: 950,\n",
      "    FP: 0,\n",
      "    FN: 460\n",
      "    accuracy: 950 / 1410 = 67.38%\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Create a neural network\n",
    "basic5_covid = BasicNeuralNetwork(\n",
    "    input_size=X.shape[1],\n",
    "    output_size=2,\n",
    "    iterations=2000,\n",
    "    logs=True,\n",
    "    log_frequency=200,\n",
    "    alpha=1,\n",
    "    alpha_decay=0.001,\n",
    "    layer_size=5\n",
    ")\n",
    "\n",
    "basic5_covid.learn(X, y)\n",
    "basic5_covid_confusion_matrix = util.evaluate_custom(X, y, basic5_covid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At just 5 neurons in the hidden layer, the network simply reduces the problem to the majority class. Let's try increasing the number of neurons and compare results. Keep in mind, that the neural network uses **Stochastic Gradient Descent** optimization (same for previous examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss: 2.862640055254513, accuracy: 0.6716312056737589, learning_rate: 1.0\n",
      "2000: loss: 0.33949805716523584, accuracy: 0.8191489361702128, learning_rate: 0.3333333333333333\n",
      "4000: loss: 0.2568380210135458, accuracy: 0.8666666666666667, learning_rate: 0.2\n",
      "6000: loss: 0.23237158410664194, accuracy: 0.8851063829787233, learning_rate: 0.14285714285714285\n",
      "8000: loss: 0.21459464063490916, accuracy: 0.8971631205673759, learning_rate: 0.1111111111111111\n",
      "10000: loss: 0.2041087678660211, accuracy: 0.9, learning_rate: 0.09090909090909091\n",
      "12000: loss: 0.19730943899444436, accuracy: 0.9035460992907801, learning_rate: 0.07692307692307693\n",
      "14000: loss: 0.19220216077105673, accuracy: 0.9070921985815603, learning_rate: 0.06666666666666667\n",
      "16000: loss: 0.1884859330194672, accuracy: 0.9106382978723404, learning_rate: 0.058823529411764705\n",
      "18000: loss: 0.18472467790284847, accuracy: 0.9099290780141844, learning_rate: 0.05263157894736842\n",
      "Finished learning. Accuracy: 0.9141843971631206.\n",
      "=======================\n",
      "RESULTS:\n",
      "\n",
      "    TP: 437,\n",
      "    TN: 851,\n",
      "    FP: 99,\n",
      "    FN: 23\n",
      "    accuracy: 1288 / 1410 = 91.35%\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Create a neural network\n",
    "basic64_covid = BasicNeuralNetwork(\n",
    "    input_size=X.shape[1],\n",
    "    output_size=2,\n",
    "    iterations=20000,\n",
    "    logs=True,\n",
    "    log_frequency=2000,\n",
    "    alpha=1,\n",
    "    alpha_decay=0.001,\n",
    "    layer_size=64\n",
    ")\n",
    "\n",
    "basic64_covid.learn(X, y)\n",
    "basic64_covid_confusion_matrix = util.evaluate_custom(X, y, basic64_covid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve accuracy of about 80%, but there are a lot of false negatives which is, due to the nature of our problem, very important. It is better to have higher specificty and catch every admission with a few false admissions, than miss-diagnose patients. Let's try AdaGrad optimizer and increase compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss: 2.716393144741217, accuracy: 0.6546099290780142, learning_rate: 1.0\n",
      "2000: loss: 0.3792006875480715, accuracy: 0.8148936170212766, learning_rate: 0.3333333333333333\n",
      "4000: loss: 0.3164302512050618, accuracy: 0.8560283687943262, learning_rate: 0.2\n",
      "6000: loss: 0.29087622899170645, accuracy: 0.8702127659574468, learning_rate: 0.14285714285714285\n",
      "8000: loss: 0.28636079598423053, accuracy: 0.8709219858156029, learning_rate: 0.1111111111111111\n",
      "10000: loss: 0.2728250405552108, accuracy: 0.8815602836879433, learning_rate: 0.09090909090909091\n",
      "12000: loss: 0.2807012215189236, accuracy: 0.875177304964539, learning_rate: 0.07692307692307693\n",
      "14000: loss: 0.26603078069989117, accuracy: 0.8836879432624114, learning_rate: 0.06666666666666667\n",
      "16000: loss: 0.2594751296869806, accuracy: 0.8858156028368794, learning_rate: 0.058823529411764705\n",
      "18000: loss: 0.2561534230187364, accuracy: 0.8886524822695036, learning_rate: 0.05263157894736842\n",
      "Finished learning. Accuracy: 0.8921985815602836.\n",
      "=======================\n",
      "RESULTS:\n",
      "\n",
      "    TP: 345,\n",
      "    TN: 908,\n",
      "    FP: 42,\n",
      "    FN: 115\n",
      "    accuracy: 1253 / 1410 = 88.87%\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Create a neural network\n",
    "adagrad64_covid = BasicNeuralNetwork(\n",
    "    input_size=X.shape[1],\n",
    "    output_size=2,\n",
    "    iterations=20000,\n",
    "    logs=True,\n",
    "    log_frequency=2000,\n",
    "    alpha=1,\n",
    "    alpha_decay=0.001,\n",
    "    layer_size=64,\n",
    "    optimizer=\"adagrad\"\n",
    ")\n",
    "\n",
    "adagrad64_covid.learn(X, y)\n",
    "adagrad64_covid_confusion_matrix = util.evaluate_custom(X, y, adagrad64_covid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad optimizer reduced the number of false negatives by quite a lot, although it increased the number of false positives. Let's try Adam optimizer as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss: 10.18578294916905, accuracy: 0.32269503546099293, learning_rate: 1.0\n",
      "2000: loss: 0.3911959045253613, accuracy: 0.8085106382978723, learning_rate: 0.3333333333333333\n",
      "4000: loss: 0.3392113341770831, accuracy: 0.8390070921985816, learning_rate: 0.2\n",
      "6000: loss: 0.3133103012013904, accuracy: 0.850354609929078, learning_rate: 0.14285714285714285\n",
      "8000: loss: 0.30068975648881446, accuracy: 0.8659574468085106, learning_rate: 0.1111111111111111\n",
      "10000: loss: 0.28714804843759195, accuracy: 0.8645390070921986, learning_rate: 0.09090909090909091\n",
      "12000: loss: 0.2771837209642236, accuracy: 0.8765957446808511, learning_rate: 0.07692307692307693\n",
      "14000: loss: 0.2707767626657338, accuracy: 0.8765957446808511, learning_rate: 0.06666666666666667\n",
      "16000: loss: 0.2645394363731891, accuracy: 0.8780141843971632, learning_rate: 0.058823529411764705\n",
      "18000: loss: 0.2626652150300176, accuracy: 0.8829787234042553, learning_rate: 0.05263157894736842\n",
      "Finished learning. Accuracy: 0.8836879432624114.\n",
      "=======================\n",
      "RESULTS:\n",
      "\n",
      "    TP: 403,\n",
      "    TN: 844,\n",
      "    FP: 106,\n",
      "    FN: 57\n",
      "    accuracy: 1247 / 1410 = 88.44%\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Create a neural network\n",
    "adam64_covid = BasicNeuralNetwork(\n",
    "    input_size=X.shape[1],\n",
    "    output_size=2,\n",
    "    iterations=20000,\n",
    "    logs=True,\n",
    "    log_frequency=2000,\n",
    "    alpha=1,\n",
    "    alpha_decay=0.001,\n",
    "    layer_size=64,\n",
    "    optimizer=\"adam\"\n",
    ")\n",
    "\n",
    "adam64_covid.learn(X, y)\n",
    "adam64_covid_confusion_matrix = util.evaluate_custom(X, y, adam64_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'src.utilities' has no attribute 'visualize_custom'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r9/sv_b_2_544sf_v18wvc8kr080000gn/T/ipykernel_48110/2494429091.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize_custom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madam64_covid_confusion_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'src.utilities' has no attribute 'visualize_custom'"
     ]
    }
   ],
   "source": [
    "util.visualize_custom(adam64_covid_confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb0554d9c886063c2300e6d29d1b971f9021cb873aeef0a3a001aed34f0acc3f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
