{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting COVID-19 ICU Admission Using Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "The original dataset is under **Attribution-NonCommercial 4.0 (International CC BY-NC 4.0)** license.\n",
    "\n",
    "Dataset is free to **share** and **adapt** under the following terms:\n",
    "\n",
    "* credit to the original article is given and any changes are indicated (nothing has been changed as of 30/11/2021)\n",
    "* material is not used for commercial purposes \n",
    "\n",
    "Credit:\n",
    "\n",
    "* original material is published on [Kaggle](https://www.kaggle.com/) and accessible [here](https://www.kaggle.com/S%C3%ADrio-Libanes/covid19)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "This repository contains source code and report for a seminar paper in the context of the course *Machine Learning* in the winter semester 2021/2021 at Faculty of Computer and Information science, University of Ljubljana.\n",
    "\n",
    "The dataset contains anonymized data from Hospital Sírio-Libanês, São Paulo and Brasilia. \n",
    "\n",
    "### Context (*copied from the above-mentioned Kaggle article*)\n",
    "COVID-19 pandemic impacted the whole world, overwhelming healthcare systems - unprepared for such intense and lengthy request for ICU beds, professionals, personal protection equipment and healthcare resources.\n",
    "Brazil recorded first COVID-19 case on February 26 and reached community transmission on March 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Predict admission to the ICU of confirmed COVID-19 cases.\n",
    "Based on the data available, is it feasible to predict which patients will need intensive care unit support?\n",
    "The aim is to provide tertiary and quarternary hospitals with the most accurate answer, so ICU resources can be arranged or patient transfer can be scheduled (*copied from Kaggle article*).\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Data has been cleaned and scaled by column according to Min Max Scaler. In total, there are 54 features (expanded when pertinent to the mean, median, max, min, diff and relative diff). \n",
    "\n",
    "### Available Data\n",
    "\n",
    "Features in the dataset can be grouped in four groups.\n",
    "\n",
    "\n",
    "\n",
    "| Group | Amount of features |\n",
    "| ----- | :------------------: |\n",
    "| Demographics | 3 |\n",
    "| Grouped diseases | 9 |\n",
    "| Blood results | 36 |\n",
    "| Vital signs | 6 |\n",
    "| **Total**| **54** |\n",
    "\n",
    "\n",
    "\n",
    "### Window Concept\n",
    "\n",
    "Data for each patient has been grouped in five windows, each containing diagnostic results from the respective time window.\n",
    "\n",
    "\n",
    "\n",
    "| Window      | Description |\n",
    "| ----------- | ----------- |\n",
    "| 0-2         | From 0 to 2 hours of the admission |\n",
    "| 2-4         | From 2 to 4 hours of the admission |\n",
    "| 4-6         | From 4 to 6 hours of the admission |\n",
    "| 6-12        | From 6 to 12 hours of the admission |\n",
    "| Above 12    | Above 12 hours from admission |\n",
    "\n",
    "\n",
    "\n",
    "Kaggle article warns not to use data from the window where the target variable is 1. This means we need to manipulate our data a little. For example let's take a look at the following time tables:\n",
    "\n",
    "\n",
    "\n",
    "| Window      | Patient admitted to ICU | Data can be used for modelling | Target variable |\n",
    "| ----------- | :-----------: | :-----------: | :-----------: |\n",
    "| 0-2         | False | True | 1 |\n",
    "| 2-4         | False | True | 1 |\n",
    "| 4-6         | False | True | 1 |\n",
    "| 6-12        | True | False |  |\n",
    "| Above 12    | True | False |  |\n",
    "\n",
    "\n",
    "\n",
    "Patient is admitted in the fourth time window (6-12 from initial non-ICU admission). This means we can use data from the first three time windows with target variable being 1 (patient being admitted to the ICU ward).\n",
    "\n",
    "\n",
    "\n",
    "| Window      | Patient admitted to ICU | Data can be used for modelling | Target variable |\n",
    "| ----------- | :-----------: | :-----------: | :-----------: |\n",
    "| 0-2         | False | True | 0 |\n",
    "| 2-4         | False | True | 0 |\n",
    "| 4-6         | False | True | 0 |\n",
    "| 6-12        | False | True | 0 |\n",
    "| Above 12    | False | True | 0 |\n",
    "\n",
    "\n",
    "\n",
    "Patient is never admitted to the ICU, we can therefore use all time windows with target variable 0.\n",
    "\n",
    "### Null Values\n",
    "\n",
    "If we take a look at the following snippet from the original Kaggle article:\n",
    "\n",
    "```\n",
    "It is reasonable to assume that a patient who does not have a measurement recorded in a time window is clinically stable, potentially presenting vital signs and blood labs similar to neighboring windows. Therefore, one may fill the missing values using the next or previous entry. Attention to multicollinearity and zero variance issues in this data when choosing your algorithm.\n",
    "```\n",
    "\n",
    "We will be filling missing values from neighbouring cells, as specified in the snippet above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Before you begin to run your code, you need to load all required modules. Simply execute the code block below. This block also enables Jupyter's auto-reloading feature, so you dont need to re-import modules whenever you change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# In order to import from the python file without hassle, we add the current\n",
    "# directory to the python path\n",
    "import sys; sys.path.append(\".\")\n",
    "\n",
    "# Auto-reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Utilities module\n",
    "import src.utilities as util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Instructions on how to set up the environment are specified in the [README](https://github.com/JakobSkornik/covid19-admission/blob/main/README.md) file.\n",
    "\n",
    "The original dataset is provided in a single *xlsx* file. Let us first import the dataset and store it in a single pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Excel into DataFrame\n",
    "dataset = util.load_xlsx(\"data/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx\")\n",
    "\n",
    "# Print first 10 elements of DataFrame\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the datatypes present in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we want to do, is to add the target variable to all the rows. If there's at least one positive value in the **ICU** column for a single patient, the target variable is 1.\n",
    "\n",
    "First we obtain the target variable for every patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df with PATIENT_ID/TARGET columns\n",
    "patient_target_df = util.get_target_variables(dataset)\n",
    "patient_target_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we append target variable to each row of the **dataset** dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = util.append_target_variable(dataset, patient_target_df)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can remove the rows, where dataset contains value 1 in the column **ICU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset.ICU != 1]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we remove metadata column containing patient ID **PATIENT_VISIT_IDENTIFIER** and column **ICU**, since every row has the same ICU value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop([\"PATIENT_VISIT_IDENTIFIER\", \"ICU\"], axis=1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have to deal with null values. As specified above, we will fill null values with neighbouring values. We can take advantage of the **pd.DataFrame.fillna** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_backward_fill = dataset.fillna(method=\"bfill\")\n",
    "dataset_forward_fill = dataset.fillna(method=\"ffill\")\n",
    "\n",
    "backward_filled = dataset_backward_fill.isna().sum().all()\n",
    "forward_filled = dataset_forward_fill.isna().sum().all()\n",
    "\n",
    "backward_filled, forward_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods successfully filled the dataset. We can select either one of those, or save them to compare results between them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_backward_fill\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still whitespace characters in column names, so we replace them with underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns = dataset.columns.str.replace(\" \", \"_\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to encode the **AGE_PERCENTIL** column. We can see that there are 10 distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.AGE_PERCENTIL.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use pandas.get_dummies method, which will map a single column with n possible values, into n different binary columns. Column representing the original value, will contain value 1.\n",
    "\n",
    "For example the following column:\n",
    "\n",
    "| Value |\n",
    "| :----: |\n",
    "| a |\n",
    "| b |\n",
    "| c |\n",
    "| a |\n",
    "| a |\n",
    "\n",
    "will get mapped into:\n",
    "\n",
    "| Value_a | Value_b | Value_c |\n",
    "| :---: | :---: | :---: |\n",
    "| 1 | 0 | 0 |\n",
    "| 0 | 1 | 0 |\n",
    "| 0 | 0 | 1 |\n",
    "| 1 | 0 | 0 |\n",
    "| 1 | 0 | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = util.get_dummies(dataset, cols=[\"AGE_PERCENTIL\"])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final non-numeric column is the **WINDOW** column. Using this column, we can create 6 different datasets. There are five windows, so we can use each distinct value as a separate dataset and an additional dataset with all time windows. I will show the example for window 2-4. Keep in mind, when we create a dataset for a window, all previous windows must be included aswell.\n",
    "\n",
    "After we extract a dataset for a desired window, **WINDOW** column can be dropped.\n",
    "\n",
    "All datasets will then be created and stored in a new object, with all required metadata using a helper method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_24_dataset = dataset[(dataset.WINDOW == \"0-2\") | (dataset.WINDOW == \"2-4\")]\n",
    "window_24_dataset = window_24_dataset.drop(\"WINDOW\", axis=1)\n",
    "window_24_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make sure that the datatypes of the curated dataset are all numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_24_dataset.dtypes.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply this data preparation again, this time for all time windows and for different fill methods separately in a script. The final dictionary is structured as follows:\n",
    "\n",
    "* **datasets**: *dict*\n",
    "  * **ffill_datasets**: *dict*\n",
    "    * **window_0_2**: *pd.Dataframe*\n",
    "    * **window_2_4**: *pd.DataFrame*\n",
    "    * **window_4_6**: *pd.DataFrame*\n",
    "    * **window_6_12**: *pd.DataFrame*\n",
    "    * **window_all**: *pd.DataFrame*\n",
    "  * **bfill_datasets**: *dict*\n",
    "    * . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = util.get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "The first question as to why one would want to design a neural network by hand, can be answered easily; to understand how such algorithms work at a deep level and because it's fun.\n",
    "\n",
    "The whole neural network is designed as a package, that we can use in this notebook. We will start with a simple stochastic gradient descent backpropagation algorithm.\n",
    "\n",
    "The first step is to desgin a simple neural network thats capable of approximating some basic boolean functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to import from the python file without hassle, we add the current\n",
    "# directory to the python path\n",
    "import sys; sys.path.append(\".\")\n",
    "\n",
    "# Auto-reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.neural_network.basic import BasicNeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a neural network\n",
    "nn = BasicNeuralNetwork(\n",
    "    input_size=2,\n",
    "    output_size=2,\n",
    "    iterations=1000,\n",
    "    log_frequency=100,\n",
    "    alpha=1,\n",
    "    alpha_decay=0.05,\n",
    "    layer_size=64,\n",
    "    hidden_layers=1,\n",
    ")\n",
    "\n",
    "# Create dataset\n",
    "X = [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "]\n",
    "\n",
    "# AND target values\n",
    "y = [0, 0, 0, 1]\n",
    "\n",
    "# Train on the dataset\n",
    "nn.learn(X, y)\n",
    "\n",
    "# Optional encoding\n",
    "encoding = {\n",
    "    0: \"False\",\n",
    "    1: \"True\"\n",
    "}\n",
    "\n",
    "# Predictions\n",
    "X00 = [[0,0]]\n",
    "X01 = [[0,1]]\n",
    "X10 = [[1,0]]\n",
    "X11 = [[1,1]]\n",
    "\n",
    "# Results\n",
    "print(\"\\nRESULTS: \")\n",
    "print(f\"\\tFalse and False -> {nn.predict(X00, encoding)}\")\n",
    "print(f\"\\tFalse and True  -> {nn.predict(X01, encoding)}\")\n",
    "print(f\"\\tTrue and False  -> {nn.predict(X10, encoding)}\")\n",
    "print(f\"\\tTrue and True   -> {nn.predict(X11, encoding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network seems to work for an AND method. Let's try XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss: 3.532179491345712, accuracy: 0.5, learning_rate: 1.0\n",
      "1: loss: 8.059047875479163, accuracy: 0.5, learning_rate: 0.9990009990009991\n",
      "2: loss: 0.745304719745516, accuracy: 0.75, learning_rate: 0.998003992015968\n",
      "3: loss: 0.6814844140854559, accuracy: 0.5, learning_rate: 0.9970089730807579\n",
      "4: loss: 1.0256145821789489, accuracy: 0.5, learning_rate: 0.9960159362549801\n",
      "5: loss: 0.6206621311196778, accuracy: 0.5, learning_rate: 0.9950248756218907\n",
      "6: loss: 0.23299459053884802, accuracy: 1.0, learning_rate: 0.9940357852882704\n",
      "7: loss: 0.13498562804541667, accuracy: 1.0, learning_rate: 0.99304865938431\n",
      "8: loss: 0.11846899228654595, accuracy: 1.0, learning_rate: 0.9920634920634921\n",
      "9: loss: 0.10588533391221254, accuracy: 1.0, learning_rate: 0.9910802775024778\n",
      "Finished learning. Accuracy: 1.0.\n",
      "\n",
      "RESULTS: \n",
      "\tFalse and False -> False\n",
      "\tFalse and True  -> True\n",
      "\tTrue and False  -> True\n",
      "\tTrue and True   -> False\n"
     ]
    }
   ],
   "source": [
    "# Create a neural network\n",
    "nn = BasicNeuralNetwork(\n",
    "    input_size=2,\n",
    "    output_size=2,\n",
    "    iterations=10,\n",
    "    logs=True,\n",
    "    log_frequency=1,\n",
    "    alpha=1,\n",
    "    alpha_decay=0.001,\n",
    "    layer_size=64,\n",
    "    hidden_layers=1,\n",
    ")\n",
    "\n",
    "# Create dataset\n",
    "X = [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "]\n",
    "\n",
    "# XOR target values\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "# Train on the dataset\n",
    "nn.learn(X, y)\n",
    "\n",
    "# Optional encoding\n",
    "encoding = {\n",
    "    0: \"False\",\n",
    "    1: \"True\"\n",
    "}\n",
    "\n",
    "# Predictions\n",
    "X00 = [[0,0]]\n",
    "X01 = [[0,1]]\n",
    "X10 = [[1,0]]\n",
    "X11 = [[1,1]]\n",
    "\n",
    "# Results\n",
    "print(\"\\nRESULTS: \")\n",
    "print(f\"\\tFalse and False -> {nn.predict(X00, encoding)}\")\n",
    "print(f\"\\tFalse and True  -> {nn.predict(X01, encoding)}\")\n",
    "print(f\"\\tTrue and False  -> {nn.predict(X10, encoding)}\")\n",
    "print(f\"\\tTrue and True   -> {nn.predict(X11, encoding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1410,229) and (1410,64) not aligned: 229 (dim 1) != 1410 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r9/sv_b_2_544sf_v18wvc8kr080000gn/T/ipykernel_45386/3529449929.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/School/ML/covid19-admission/src/neural_network/basic.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, dataset, truths)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/School/ML/covid19-admission/src/neural_network/basic.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, val, truths)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/School/ML/covid19-admission/src/neural_network/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;34m\"\"\"Forward pass.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/School/ML/covid19-admission/src/neural_network/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1410,229) and (1410,64) not aligned: 229 (dim 1) != 1410 (dim 0)"
     ]
    }
   ],
   "source": [
    "dataset = datasets[\"ffill_datasets\"][\"window_all\"].copy()\n",
    "\n",
    "X = dataset.drop(\"TARGET\", axis=1).to_numpy()\n",
    "y = dataset[[\"TARGET\"]].to_numpy()\n",
    "\n",
    "# Create a neural network\n",
    "nn = BasicNeuralNetwork(\n",
    "    input_size=X.shape[1],\n",
    "    output_size=2,\n",
    "    iterations=1000,\n",
    "    logs=True,\n",
    "    log_frequency=100,\n",
    "    alpha=1,\n",
    "    alpha_decay=0.001,\n",
    "    layer_size=64,\n",
    "    hidden_layers=1,\n",
    ")\n",
    "\n",
    "nn.learn(X, y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb0554d9c886063c2300e6d29d1b971f9021cb873aeef0a3a001aed34f0acc3f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
